version: '3.8'

services:
  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=minio123
    ports:
      - "9000:9000" 
      - "9001:9001"  
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      - iceberg_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # minio-init:
  #   image: minio/mc
  #   container_name: minio-init
  #   depends_on:
  #     minio:
  #       condition: service_healthy
  #   volumes:
  #     - ./dag/dbt_quang/seeds/data:/data
  #   networks:
  #     - iceberg_net
  #   entrypoint: >
  #     /bin/sh -c "
  #     set -e;
  #     echo 'Configuring MinIO client...';
  #     /usr/bin/mc alias set localminio http://minio:9000 minio minio123;
  #     echo 'Configuration successful. Creating buckets...';
  #     /usr/bin/mc ls localminio/raw > /dev/null 2>&1 || /usr/bin/mc mb localminio/raw;
  #     /usr/bin/mc ls localminio/warehouse > /dev/null 2>&1 || /usr/bin/mc mb localminio/warehouse;
  #     echo 'Buckets creation process finished.';
  #     echo 'Uploading initial data...';
  #     /usr/bin/mc cp --recursive /data/ localminio/raw/;
  #     echo 'Initial data uploaded successfully.';
  #     "

  #Database cho Hive Metastore
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_DB=metastore
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_CONFIG_FILE=/var/lib/postgresql/data/postgresql.conf
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./logs/postgres:/var/log/postgresql
      - ./config/postgres/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    networks:
      - iceberg_net

  #Dịch vụ Hive Metastore
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    environment:
      - HIVE_DBTYPE=postgres
      - SERVICE_NAME=metastore
      - HIVE_JDBC_URL=jdbc:postgresql://postgres:5432/metastore
      - HIVE_LOG4J2_CONF_FILE=/opt/hive/conf/log4j2.properties
      - HIVE_OPTS=-Dlog4j.configurationFile=file:/opt/hive/conf/log4j2.properties
    ports:
      - "9083:9083"
    depends_on:
      - postgres
    volumes:
      - ./jars/aws-java-sdk-bundle-1.12.367.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.367.jar
      - ./jars/hadoop-aws-3.3.4.jar:/opt/hive/lib/hadoop-aws-3.3.4.jar
      - ./jars/postgresql-42.2.27.jar:/opt/hive/lib/postgresql-42.2.27.jar
      - ./config/hive/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./logs/hive:/var/log
      - ./config/hive/hive-log4j2.properties:/opt/hive/conf/hive-log4j2.properties
      - ./config/hive/start-metastore.sh:/opt/hive/start-metastore.sh
    entrypoint: bash /opt/hive/start-metastore.sh
    networks:
      - iceberg_net

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
    ports:
      - "8080:8080" 
      - "7077:7077"
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./jars/aws-java-sdk-bundle-1.12.367.jar:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.367.jar
      - ./jars/hadoop-aws-3.3.4.jar:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar
      - ./jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar:/opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
      - ./jars/postgresql-42.2.27.jar:/opt/bitnami/spark/jars/postgresql-42.2.27.jar
      - ./jars/spark-sql-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar
      - ./jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar
      - ./jars/kafka-clients-3.5.1.jar:/opt/bitnami/spark/jars/kafka-clients-3.5.1.jar
      - ./jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
      - ./logs/spark/master:/opt/bitnami/spark/logs
      - ./config/spark/master/log4j2.properties:/opt/bitnami/spark/conf/log4j2.properties

    networks:
      - iceberg_net

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    depends_on:
      - spark-master
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./jars/aws-java-sdk-bundle-1.12.367.jar:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.367.jar
      - ./jars/hadoop-aws-3.3.4.jar:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar
      - ./jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar:/opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
      - ./jars/postgresql-42.2.27.jar:/opt/bitnami/spark/jars/postgresql-42.2.27.jar
      - ./jars/spark-sql-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar
      - ./jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar
      - ./jars/kafka-clients-3.5.1.jar:/opt/bitnami/spark/jars/kafka-clients-3.5.1.jar
      - ./jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
      - ./logs/spark/worker:/opt/bitnami/spark/logs
      - ./config/spark/worker/log4j2.properties:/opt/bitnami/spark/conf/log4j2.properties
    networks:
      - iceberg_net
  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    depends_on:
      - spark-master
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./jars/aws-java-sdk-bundle-1.12.367.jar:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.367.jar
      - ./jars/hadoop-aws-3.3.4.jar:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar
      - ./jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar:/opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
      - ./jars/postgresql-42.2.27.jar:/opt/bitnami/spark/jars/postgresql-42.2.27.jar
      - ./jars/spark-sql-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar
      - ./jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar
      - ./jars/kafka-clients-3.5.1.jar:/opt/bitnami/spark/jars/kafka-clients-3.5.1.jar
      - ./jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
      - ./logs/spark/worker:/opt/bitnami/spark/logs
      - ./config/spark/worker/log4j2.properties:/opt/bitnami/spark/conf/log4j2.properties
    networks:
      - iceberg_net

  spark-thrift-server:
    image: bitnami/spark:3.5.0
    container_name: spark-thrift-server
    environment:
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_MASTER_URL=spark://spark-master:7077
    command: >
      /bin/bash -c "
      /opt/bitnami/spark/sbin/start-thriftserver.sh --master spark://spark-master:7077 
      --conf hive.server2.thrift.port=10000 
      --conf spark.sql.adaptive.enabled=true 
      --conf spark.sql.adaptive.localShuffleReader.enabled=true
      --conf spark.dynamicAllocation.enabled=false
      --conf spark.executor.instances=1
      --conf spark.executor.cores=2 
      --conf spark.executor.memory=1g 
      --conf spark.driver.memory=512m 
      --conf spark.cores.max=2
      && sleep infinity"
    ports:
      - "10000:10000" # Port cho dbt kết nối
      - "4041:4040"   # UI cho thrift server
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./jars/aws-java-sdk-bundle-1.12.367.jar:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.367.jar
      - ./jars/hadoop-aws-3.3.4.jar:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar
      - ./jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar:/opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
      - ./jars/postgresql-42.2.27.jar:/opt/bitnami/spark/jars/postgresql-42.2.27.jar
      - ./jars/spark-sql-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar
      - ./jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar
      - ./jars/kafka-clients-3.5.1.jar:/opt/bitnami/spark/jars/kafka-clients-3.5.1.jar
      - ./jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar:/opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar
      - ./logs/spark/thrift:/opt/bitnami/spark/logs
      - ./config/spark/thrift/log4j2.properties:/opt/bitnami/spark/conf/log4j2.properties
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
    depends_on:
      - spark-master
      - hive-metastore
    networks:
      - iceberg_net

  # coordinator:
  #   image: trinodb/trino:436
  #   container_name: trino-coordinator
  #   ports:
  #     - "7000:7000"
  #   volumes:
  #     - ./config/trino/coordinator/etc:/etc/trino
  #     - ./config/trino/coordinator/data:/var/trino/data
  #   environment:
  #     - JAVA_TOOL_OPTIONS=-Xmx2G
  #   networks:
  #     - iceberg_net
  # worker1:
  #   image: trinodb/trino:436
  #   container_name: trino-worker1
  #   volumes:
  #     - ./config/trino/worker1/etc:/etc/trino
  #     - ./config/trino/worker1/data:/var/trino/data
  #   depends_on:
  #     - coordinator
  #   environment:
  #     - JAVA_TOOL_OPTIONS=-Xmx2G
  #   networks:
  #     - iceberg_net
  # worker2:
  #   image: trinodb/trino:436
  #   container_name: trino-worker2
  #   volumes:
  #     - ./config/trino/worker2/etc:/etc/trino
  #     - ./config/trino/worker2/data:/var/trino/data
  #   depends_on:
  #     - coordinator
  #   environment:
  #     - JAVA_TOOL_OPTIONS=-Xmx2G
  #   networks:
  #     - iceberg_net
  redis:
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - iceberg_net

  postgres-airflow:
    image: postgres:13
    container_name: postgres-airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - iceberg_net

  # airflow-init:
  #   image: airflow-java
  #   container_name: airflow-init
  #   depends_on:
  #     postgres-airflow:
  #       condition: service_healthy
  #     redis:
  #       condition: service_healthy
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
  #     - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
  #     - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
  #     - AIRFLOW__CORE__FERNET_KEY=z_lL_eTlP5BTuFAYKS8t4sqVfh97hySVN3PW4NCcwy4=
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #     - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
  #     - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements.txt
  #     - AIRFLOW_CONN_DBT_SPARK=spark://spark-thrift-server:10000/default
  #     - AIRFLOW_ADMIN_PASSWORD=${AIRFLOW_ADMIN_PASSWORD}
  #   volumes:
  #     - ./dag:/opt/airflow/dags
  #     - ./logs/airflow:/opt/airflow/logs
  #     - ./config/airflow/plugins:/opt/airflow/plugins
  #     - ./config/airflow/requirements.txt:/opt/airflow/requirements.txt
  #     - ./app:/opt/airflow/app
  #     - ./app:/opt/bitnami/spark/app
  #     - ./jars:/opt/airflow/jars
  #   command: >
  #     bash -c "pip install --user -r /opt/airflow/requirements.txt &&
  #     pip install dbt-spark[PyHive] &&
  #     echo 'Waiting for PostgreSQL to be ready...' &&
  #     sleep 5 &&
  #     airflow db migrate &&
  #     echo 'Setting Airflow Variable: processing_date' &&
  #     airflow variables set processing_date 2014-09-07 &&
  #     echo 'Deleting and Re-adding connection dbt_spark to ensure it is fresh' &&
  #     (airflow connections delete 'dbt_spark' || true) && airflow connections add 'dbt_spark' --conn-uri 'spark://spark-thrift-server:10000/default?extra=%7B%22hive.metastore.uris%22%3A%22thrift%3A%2F%2Fhive-metastore%3A9083%22%2C%22queue%22%3A%22default%22%7D' &&
  #     echo 'Creating user admin...' &&
  #     airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password \"${AIRFLOW_ADMIN_PASSWORD}\""
  #   networks:
  #     - iceberg_net

  airflow-webserver:
    image: airflow-java
    container_name: airflow-webserver
    depends_on:
      # airflow-init:
      #   condition: service_completed_successfully
      postgres-airflow:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=z_lL_eTlP5BTuFAYKS8t4sqVfh97hySVN3PW4NCcwy4=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements.txt
      - AIRFLOW_CONN_DBT_SPARK=spark://spark-thrift-server:10000/default
    volumes:
      - ./dag:/opt/airflow/dags
      - ./logs/airflow:/opt/airflow/logs
      - ./config/airflow/plugins:/opt/airflow/plugins
      - ./config/airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
      - ./jars:/opt/airflow/jars
    ports:
      - "8900:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      - iceberg_net

  airflow-scheduler:
    image: airflow-java
    container_name: airflow-scheduler
    depends_on:
      # airflow-init:
      #   condition: service_completed_successfully
      postgres-airflow: 
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=z_lL_eTlP5BTuFAYKS8t4sqVfh97hySVN3PW4NCcwy4=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements.txt
      - AIRFLOW_CONN_DBT_SPARK=spark://spark-thrift-server:10000/default
    volumes:
      - ./dag:/opt/airflow/dags
      - ./logs/airflow:/opt/airflow/logs
      - ./config/airflow/plugins:/opt/airflow/plugins
      - ./config/airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
      - ./jars:/opt/airflow/jars
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --local"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      - iceberg_net

  airflow-worker:
    image: airflow-java
    container_name: airflow-worker
    depends_on:
      # airflow-init:
      #   condition: service_completed_successfully
      postgres-airflow:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=z_lL_eTlP5BTuFAYKS8t4sqVfh97hySVN3PW4NCcwy4=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements.txt
      - AIRFLOW_CONN_DBT_SPARK=spark://spark-thrift-server:10000/default
    volumes:
      - ./dag:/opt/airflow/dags
      - ./logs/airflow:/opt/airflow/logs
      - ./config/airflow/plugins:/opt/airflow/plugins
      - ./config/airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./app:/opt/airflow/app
      - ./app:/opt/bitnami/spark/app
      - ./jars:/opt/airflow/jars
    command: celery worker
    healthcheck:
      test: ["CMD-SHELL", "celery --app airflow.executors.celery_executor.app inspect ping -d celery@$HOSTNAME"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      - iceberg_net
volumes:
  minio_data:
  postgres_data:
  postgres_airflow_data:
networks:
  iceberg_net: